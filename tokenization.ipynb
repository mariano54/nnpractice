{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-08T19:14:46.552726Z",
     "start_time": "2024-07-08T19:14:46.487833Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from functools import lru_cache\n",
    "from typing import Dict, List, Tuple, Set\n",
    "import regex\n",
    "import json\n",
    "\n",
    "def bpe_train(dataset: str, num_merges: int) -> Dict[int, str]:\n",
    "    pass\n",
    "\n",
    "def read_corpus(filename: str) -> Dict[int, str]:\n",
    "    with open(filename, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = f.read().encode(\"utf-8\")\n",
    "\n",
    "read_corpus(\"data/taylor_swift_wiki.txt\")\n",
    "\n",
    "encoding_map: Dict[str, int] = {}\n",
    "\n",
    "with open(\"weights/gpt2_encoder.json\", \"rb\") as f:\n",
    "    for k, v in json.loads(f.read()).items():\n",
    "        encoding_map[k] = v\n",
    "\n",
    "with open(\"weights/gpt2_vocab.bpe\", \"rb\") as f:\n",
    "    merges: Dict[Tuple[str, str], int] = {tuple(l[:-1].decode(\"utf-8\").split(' ')): i+256 for i, l in enumerate(f.readlines()[1:])}\n",
    " \n",
    "\n"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-08T19:29:11.240604Z",
     "start_time": "2024-07-08T19:29:11.220011Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_stats(unicode_str: List[str]) -> Dict[Tuple[str, str], int]:\n",
    "    occurrences: Dict[Tuple[str, str], int] = {}\n",
    "    for i in range(len(unicode_str) - 1):\n",
    "        pair = (unicode_str[i], unicode_str[i + 1])\n",
    "        occurrences[pair] = occurrences.get(pair, 0) + 1\n",
    "    return occurrences\n",
    "\n",
    "\n",
    "class GPT2Tokenizer:\n",
    "    def __init__(self):\n",
    "        self.merges = merges\n",
    "        self.encoding_map = encoding_map\n",
    "        self.decoding_map = {v: k for k,v in encoding_map.items()}\n",
    "        self.byte_encoding = self.bytes_to_unicode()\n",
    "        self.byte_decoding = {v: k for k,v in self.byte_encoding.items()}\n",
    "        \n",
    "    # Taken from https://github.com/openai/gpt-2/blob/master/src/encoder.py (MIT LICENSE)\n",
    "    @lru_cache()\n",
    "    def bytes_to_unicode(self):\n",
    "        bs = list(range(ord(\"!\"), ord(\"~\")+1))+list(range(ord(\"¡\"), ord(\"¬\")+1))+list(range(ord(\"®\"), ord(\"ÿ\")+1))\n",
    "        cs = bs[:]\n",
    "        n = 0\n",
    "        for b in range(2**8):\n",
    "            if b not in bs:\n",
    "                bs.append(b)\n",
    "                cs.append(2**8+n)\n",
    "                n += 1\n",
    "        cs = [chr(n) for n in cs]\n",
    "        return dict(zip(bs, cs))\n",
    "\n",
    "    def encode(self, str_to_encode: str) -> List[int]:\n",
    "        pat_str = r\"\"\"'(?:[sdmt]|ll|ve|re)| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\"\n",
    "        gpt2_regex = regex.compile(pat_str)\n",
    "        final_list: List[str] = []\n",
    "        for sub_string in gpt2_regex.findall(str_to_encode):\n",
    "            current_str: List[str] = [self.byte_encoding[b] for b in sub_string.encode(\"utf-8\")]\n",
    "            not_found_strs: Set[Tuple[str, str]]= set()\n",
    "            while True:\n",
    "                stats = get_stats(current_str)\n",
    "                large_number = 999999999\n",
    "                merge_options = [(self.merges.get((pair[0], pair[1]), large_number), pair) \n",
    "                                 for pair in stats.keys() \n",
    "                                 if pair not in not_found_strs]\n",
    "                if len(merge_options) == 0:\n",
    "                    final_list += current_str\n",
    "                    break\n",
    "                encoding_num, pair = min(merge_options)\n",
    "                if encoding_num == large_number:\n",
    "                    not_found_strs.add(pair)\n",
    "                    continue\n",
    "                new_str: List[str] = []\n",
    "                i = 0\n",
    "                while i < len(current_str):\n",
    "                    if (i + 1) < len(current_str) and (current_str[i], current_str[i+1]) == pair:\n",
    "                        new_str.append(pair[0] + pair[1])\n",
    "                        i += 2\n",
    "                    else:\n",
    "                        new_str.append(current_str[i])\n",
    "                        i += 1\n",
    "                if len(current_str) == len(new_str):\n",
    "                    not_found_strs.add(pair)\n",
    "                current_str = new_str\n",
    "        return [encoding_map[w] for w in final_list]\n",
    "\n",
    "    def decode(self, encoded: List[int]) -> str:\n",
    "        unicode_str = \"\".join(self.decoding_map[i] for i in encoded)\n",
    "        decoded_utf8 = b\"\".join([bytes([self.byte_decoding[c]]) for c in unicode_str])\n",
    "        return decoded_utf8.decode(\"utf-8\")\n",
    "            \n",
    "\n",
    "gpt2_tokenizer = GPT2Tokenizer()\n",
    "test_str = \"hello world こんにちは, 今日は \\n\\n don't say but do... !\"\n",
    "result = gpt2_tokenizer.encode(test_str)\n",
    "assert result == [31373, 995, 23294, 241, 22174, 28618, 2515, 94, 31676, 11, 220, 20015, 232, 33768, 98, 31676, 220, 628, 836, 470, 910, 475, 466, 986, 5145]\n",
    "print(result)\n",
    "print(gpt2_tokenizer.decode(result))"
   ],
   "id": "ee350390c408649c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[31373, 995, 23294, 241, 22174, 28618, 2515, 94, 31676, 11, 220, 20015, 232, 33768, 98, 31676, 220, 628, 836, 470, 910, 475, 466, 986, 5145]\n",
      "hello world こんにちは, 今日は \n",
      "\n",
      " don't say but do... !\n"
     ]
    }
   ],
   "execution_count": 30
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
